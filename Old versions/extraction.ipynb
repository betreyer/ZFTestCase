{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math \n",
    "import os\n",
    "import datetime\n",
    "from datetime import date, datetime, timedelta\n",
    "import re\n",
    "import ast\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a111c",
   "metadata": {},
   "source": [
    "## 1 Checking dimensions tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_table = pd.read_excel('Inputs/User Details.xlsx', sheet_name = None,  parse_dates=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6cf39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'pandas' in str(type(dimension_table)): \n",
    "    print(dimension_table)\n",
    "elif 'dict' in str(type(dimension_table)):\n",
    "    for dimension in dimension_table:\n",
    "        print('---------------------------------------------------------------------------------------')\n",
    "        print(dimension)\n",
    "        print('---------------------------------------------------------------------------------------')\n",
    "        print(dimension_table[dimension])\n",
    "        print('---------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = dimension_table['Accounts']\n",
    "assets = dimension_table ['Assets']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5939a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(accounts.info())\n",
    "print('--------------------------------------------')\n",
    "print(accounts.duplicated(subset='AccountID').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20282926",
   "metadata": {},
   "source": [
    "<b>So, there are duplicates</b> in primary key for Accounts table. Not good. Should be reported to developers. \n",
    "Can be resolved in many ways: \n",
    "- 1.Try to find out which AccountID should actually have each of accounts in alternative source of truth (if there is any), report to devs and to fix it for now as ad hoc. \n",
    "- 2. Drop duplicate (second one by default). \n",
    "- 3. To set it last existing id in current table +1 at least for now in case I need to calculate some stats in account table. \n",
    "\n",
    "Let's have a look on, probably, connected assets table though. \n",
    "\n",
    "Datatypes looks right. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assets.info())\n",
    "print()\n",
    "print(assets.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicator(df, primary_keys): \n",
    "    if df.duplicated(subset = primary_keys).sum() > 0: \n",
    "        df.drop_duplicates(subset = primary_keys, inplace = True, ignore_index= True)\n",
    "        return None #todo: write amount of deduplicated rows in a list -part of dict, which will be then dict of metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a51375",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicator(accounts, 'AccountID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicator(assets, 'AssetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_mapper(df, column_name, mapper):\n",
    "    new_col = column_name + '_fixed'\n",
    "    df[new_col] = df[column_name].map(mapper)\n",
    "    df[new_col] = df.apply(lambda x: x[column_name] if pd.isna(x[new_col]) else x[new_col], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcca1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_fix = {'Renaut': 'Renault'}\n",
    "values_mapper(assets, 'Make', assets_fix)\n",
    "print(assets)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.rename(columns = {'Make': \"maker\", 'Make_fixed': 'maker_fixed', 'AssetID': 'asset_id', \n",
    "                         'License Plate': 'license_plate', 'Model': 'model', 'AccountID':'account_id', \n",
    "                         'Datasource': 'data_source', 'Tank Capacity': 'tank_capacity'\n",
    "                        }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2dc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts.rename(columns= {'AccountID': 'account_id', 'AccountName':'account_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396b7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accounts.info())\n",
    "print()\n",
    "print('----------------')\n",
    "print()\n",
    "print(assets.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc463c8f",
   "metadata": {},
   "source": [
    "Talbes cleared. Let.s leave it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6837c3",
   "metadata": {},
   "source": [
    "## Facts tables cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f29e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_files_to_read(input_directory):\n",
    "    objects = os.scandir(inputs_dir)\n",
    "    fileList = []\n",
    "    for obj in objects:\n",
    "        if obj.is_file() and re.fullmatch('\\d*\\.xlsx', obj.name) is not None:\n",
    "            fileList.append(inputs_dir+obj.name)\n",
    "    return fileList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dir= 'Inputs/'\n",
    "files = fact_files_to_read(inputs_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67972131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facts_reader(files): \n",
    "    dfs= []\n",
    "    for file in files: \n",
    "        df=pd.read_excel(file, parse_dates=[1])\n",
    "        dfs.append(df)\n",
    "    return dfs \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tables = facts_reader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3429231",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact_table in fact_tables:\n",
    "    print(fact_table.columns)\n",
    "    print('-------')\n",
    "    print()   \n",
    "#Typos in column names (whitespaces), also different names for some reason. And Odometer Mts and without Mts, Pos (lat, lon)\n",
    "#and without (Lat, Long) - different formats? \n",
    "#Timestamp with format only in first file. Need to check the data and it's datatypes then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visually check data and data types. \n",
    "for fact_table in fact_tables:\n",
    "    print(fact_table.head())\n",
    "    print('-------------------')\n",
    "    print()\n",
    "    print('-------------------')\n",
    "# See different column names. Needs to be fixed. \n",
    "#also - unexpected nulls in ignition for 334455, 334458 , odometer nulls, fueel %age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5258ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tables[0].info() #some of pos are nulls, most of ignition, fueld, odometer values are nan. Need to fix or drop nulls in the future.\n",
    "#assumption: pair of ID+timestamp = natural key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1658217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fact_tables[0][' Ignition'].dropna().values) #integers, but not so many. \n",
    "print('-------------')\n",
    "print()\n",
    "print('-------------')\n",
    "print(fact_tables[0][' Odometer'].dropna().values)  # so,integers in reality, floats in table with  in the middle for some reason. \n",
    "print('-------------')\n",
    "print()\n",
    "print('-------------')\n",
    "print(fact_tables[0][' Fuel %age'].dropna().values) #integers in reality, float in table, but not so many. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c48045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_tables[0].info() #at least no nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a442f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fact_tables[1][' Ignition'].dropna().values) #ingegers\n",
    "print('-------------')\n",
    "print()\n",
    "print('-------------')\n",
    "print(fact_tables[1][' Odometer Mts'].dropna().values)  # floats \n",
    "print('-------------')\n",
    "print()\n",
    "print('-------------')\n",
    "print(fact_tables[1][' Fuel %age'].dropna().values) #integers\n",
    "\n",
    "\n",
    "#also observed all other files in the same way and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#... and now I can at least rename columns for all files and to convert datatypes to the most suitable\n",
    "#wide type: if the data in one file in column_x is int and in another file is float - it'll be float, as a type, preserving \n",
    "#accuracy of both floats and ints. \n",
    "def columns_common_names(list_df, cols_list):\n",
    "    for df in list_df: \n",
    "        df.columns = cols_list\n",
    "    return None\n",
    "# To convert cols to common datatypes, but after I can get rid of nans. \n",
    "def columns_common_datatypes(list_df, cols_convert_map):\n",
    "    for df in list_df: \n",
    "        for col in cols_convert_map:\n",
    "            df[col]=df[col].apply(cols_convert_map[col])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_cals = ['id', 'timestamp', 'ignition', 'odometer_m', 'pos', 'fuel_%']\n",
    "convert_map = {\n",
    "    'ignition': int, \n",
    "    'odometer_m': float, \n",
    "    'fuel_%': int    \n",
    "}\n",
    "\n",
    "columns_common_names(fact_tables, facts_cals)\n",
    "#columns_common_datatypes(fact_tables, convert_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact in fact_tables: \n",
    "    print(sum(fact.duplicated(subset=['id', 'timestamp']))) \n",
    "#quite unexpected. Means - id+timestamp isn't unique and \n",
    "#we actually can recieve >1 events in the same second. Can be resolved either by bigger granularity scale (milliseconds)\n",
    "#or to add events_sequence. Let's check if id+timestamp+ignition are unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact in fact_tables: \n",
    "    print(sum(fact.duplicated(subset=['id', 'timestamp', 'ignition'])))\n",
    "#yes, at least it's totally unique. But as there are null values for ignition in some tables, makes sense to fill in the values\n",
    "#based either on: previous(initial) state. Or: initial state + movement fact (to check delta of distance between rows, if \n",
    "#distance covered >0, then ignition =1, if =0 - then...well, then it can be either ignition = 0 as well as ignition = 1 (you still\n",
    "#can leave the ignited car or stay in traffic jam.)). \n",
    "\n",
    "#I think we can believe ignition sensors work, but work in different way, so in some cases they're producing results constantly,\n",
    "#and sometimes - ony during state changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = fact_tables[0]\n",
    "first_sample['is_ignition_value']=first_sample['ignition'].apply(lambda x: 0 if pd.isna(x) else 1)#could be fill na straightforward\n",
    "#but decided to implement through table functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d1203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first_sample['ignition_parts']= first_sample.sort_values('timestamp',  ascending=True).groupby(['id'])['ignition'].rank()\n",
    "\n",
    "\n",
    "\n",
    "first_sample['ignition_parts']=first_sample.sort_values('timestamp')['is_ignition_value'].cumsum()\n",
    "first_sample['ignition_filled'] = first_sample.sort_values('timestamp').groupby(['id','ignition_parts'])['ignition'].ffill()\n",
    "first_sample['ignition_filled'] = first_sample['ignition_filled'].apply(int)\n",
    "\n",
    "\n",
    "first_sample['prev_ignition'] = first_sample.sort_values('timestamp').groupby(['id'])\\\n",
    "['ignition_filled'].shift(1)\n",
    "first_sample['changed_ignition_status'] = first_sample['ignition_filled']!=first_sample['prev_ignition']\n",
    "first_sample['seq_number']=first_sample.sort_values(['timestamp', 'changed_ignition_status']).groupby(['id', 'timestamp']).cumcount()+1\n",
    "\n",
    "first_sample=first_sample[['id', 'timestamp', 'seq_number', 'ignition', 'ignition_filled', 'odometer_m', 'pos', 'fuel_%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_ignition_fill (df): \n",
    "    df['is_ignition_value']=df['ignition'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "    df['ignition_parts']= df.sort_values('timestamp')['is_ignition_value'].cumsum()\n",
    "    df['ignition_filled'] = df.sort_values('timestamp').groupby(['id','ignition_parts'])['ignition'].ffill()\n",
    "    df['ignition_filled'] = df['ignition_filled'].apply(int)\n",
    "    \n",
    "    #Sequence for the events, so engine_off should always be after engine on sent in the same time. \n",
    "    df['prev_ignition'] = df.sort_values('timestamp').groupby(['id'])['ignition_filled'].shift(1)\n",
    "    df['changed_ignition_status'] = df['ignition_filled']!=df['prev_ignition']\n",
    "    df['seq_number']=df.sort_values(['timestamp', 'changed_ignition_status']).groupby(['id', 'timestamp']).cumcount()+1\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignition_fill_all_tables(dflist):\n",
    "    for df in dflist: \n",
    "        order_and_ignition_fill(df)\n",
    "    return None\n",
    "\n",
    "ignition_fill_all_tables(fact_tables)\n",
    "for i, df in enumerate(fact_tables): \n",
    "    fact_tables[i]=fact_tables[i][['id', 'timestamp', 'seq_number', 'ignition', 'ignition_filled', 'odometer_m', 'pos', 'fuel_%']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d1b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d8cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Also it's possible to fill in coordinates and fuel, as well as odometer metric (should be nearly the same in the same moment of time) for those stop engine events, which happened the same second we have data for \n",
    "##start engine:\n",
    "def fill_in_pos_fuel_od_ign_off(df):\n",
    "    df['pos']=df.sort_values(['id','timestamp', 'seq_number']).groupby(['id', 'timestamp'])['pos'].ffill()\n",
    "    df['fuel_%']=df.sort_values(['id','timestamp', 'seq_number']).groupby(['id', 'timestamp'])['fuel_%'].ffill()\n",
    "    df['odometer_m']=df.sort_values(['id','timestamp', 'seq_number']).groupby(['id', 'timestamp'])['odometer_m'].ffill()\n",
    "    return None\n",
    "    \n",
    "def fill_same_second_fuel_pos_gaps(dflist):\n",
    "    for df in dflist:\n",
    "        fill_in_pos_fuel_ign_off(df)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179882b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_same_second_fuel_pos_gaps(fact_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b141dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in fact_tables:\n",
    "    print(df.info())\n",
    "    print()\n",
    "    print('-------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d40cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay, so ignition status is recovered. All the events have geo, but not all have odometer values. So, it makes sense to recover\n",
    "#distance covered from geo. Will be alternative source of truth for o distance covered metric, as well, as a control metric for\n",
    "#odometer. \n",
    "\n",
    "#And....let's put  latitude and longitude in separate columns too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2aa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact_tables[0]['prev_pos']=fact_tables[0].sort_values(['id', 'timestamp', 'seq_number']).groupby('id')['pos'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143513cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_extractor (string_tuple):\n",
    "    \"\"\"Allows to extract latitude and longitude from \n",
    "    stringified tuples of pos column. \n",
    "    \"\"\"\n",
    "    lat = np.nan\n",
    "    lon = np.nan\n",
    "    try: \n",
    "        lat = ast.literal_eval(string_tuple)[0]\n",
    "        lon = ast.literal_eval(string_tuple)[1]\n",
    "    except ValueError:\n",
    "        print('Input is not a tuple')\n",
    "    return lat, lon\n",
    "    \n",
    "\n",
    "\n",
    "def prev_pos (df): \n",
    "    \"\"\"Creates geo-cols in dataframe\"\"\"\n",
    "    df['prev_pos'] = df.sort_values(['id', 'timestamp', 'seq_number']).groupby('id')['pos'].shift()\n",
    "    df['current_lat'] = df['pos'].apply(lambda x: geo_extractor(x)[0])\n",
    "    df['current_lon'] = df['pos'].apply(lambda x: geo_extractor(x)[1])\n",
    "    df['prev_lat'] = df['prev_pos'].apply(lambda x: geo_extractor(x)[0])\n",
    "    df['prev_lon'] = df['prev_pos'].apply(lambda x: geo_extractor(x)[1])\n",
    "    #df['current_lon'] = df['pos'].apply(lambda x: ast.literal_eval(x)[1])\n",
    "    #df['prev_lat'] = df['prev_pos'].apply(lambda x: ast.literal_eval(x)[0])\n",
    "    #df['prev_lon'] = df['[prev_pos'].apply(lambda x: ast.literal_eval(x)[1])\n",
    "    return None\n",
    "    \n",
    "def coordinates_parser(df_list): \n",
    "    \"\"\"Iterates through the list of DFs to apply functions\"\"\"\n",
    "    for df in df_list: \n",
    "        prev_pos(df)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_parser(fact_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5894ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b8428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9e77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a266e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4cd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ex4_pandas.sort_values(\"Date\")\n",
    "    .groupby(\"ticker\")[\"closing_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f4340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a034d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fact_tables[3].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6256bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0534546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211de624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc102908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tables[1].to_csv('second_sample.csv')\n",
    "fact_tables[2].to_csv('third_sample.csv')\n",
    "fact_tables[3].to_csv('fourth_sample.csv')\n",
    "fact_tables[4].to_csv('fifth_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tables[0].to_csv('first_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd61096",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact in fact_tables: \n",
    "    order_and_ignition_fill(fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa90eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848349d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7257e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample.duplicated(subset = ['id', 'timestamp', 'seq_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195dbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a41ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample.to_csv('first_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea555a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2455e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47257500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd046d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample.columns = facts_cals\n",
    "first_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a50089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931ee2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "s=re.fullmatch('\\d*\\.xlsx', '334455.xlsx')\n",
    "if s is not None: \n",
    "    print(s.string)\n",
    "    pd.pandas.read_excel(, sheet_name=0, *, \n",
    "                         header=0, names=None, index_col=None, usecols=None, \n",
    "                         dtype=None, engine=None, converters=None, true_values=None, \n",
    "                         false_values=None, skiprows=None, nrows=None, na_values=None, \n",
    "                         keep_default_na=True, na_filter=True, verbose=False, \n",
    "                         parse_dates=False, date_parser=_NoDefault.no_default, \n",
    "                         date_format=None, thousands=None, decimal='.', comment=None, \n",
    "                         skipfooter=0, storage_options=None, dtype_backend=_NoDefault.no_default, engine_kwargs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d1167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b73e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85bf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb9ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aafef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
